{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Docling for Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import glob\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader    \n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredExcelLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import shutil\n",
    "from langchain_chroma import Chroma\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import ollama\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, Union, Optional\n",
    "import psutil\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "API_KEY = os.environ.get(\"GEMINI_API_KEY\")  \n",
    "genai.configure(api_key=API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONTEXT_PROMPT = \"\"\"\n",
    "You are an AI assistant which provides detailed and long answers.\n",
    "\n",
    "Answer the question based only on the given context.\n",
    "\n",
    "Question : {question}\n",
    "\n",
    "Conext : {context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "FORMATTING_PROMPT = '''\n",
    "Task: Extract and structure information from a query into a JSON object.\n",
    "\n",
    "Instructions:\n",
    "1. Analyze the query and extract only the following fields if they appear:\n",
    "   - \"ID\"\n",
    "   - \"name\"\n",
    "   - \"salary\"\n",
    "   - \"vacations_remaining\"\n",
    "   - \"join_date\"\n",
    "2. For any field that is not mentioned in the query, do not include it in the output.\n",
    "3. Do not include any additional fields or extra text regarding assumptions.\n",
    "\n",
    "Example 1:\n",
    "Input: Update the current salary of employee ID:1011 to 2300?\n",
    "Output: {{\"ID\": 1011, \"salary\": 2300}}\n",
    "\n",
    "Example 2:\n",
    "Input: \"Change the join date of Muhid Qaiser to 1999.\"\n",
    "Output: {{\"name\": \"Muhid Qaiser\", \"join_date\": 1999}}\n",
    "\n",
    "Example 3:\n",
    "Input: \"Can ID:1041 take a vacation tomorrow?\"\n",
    "Output: {{ \"ID\": 1011, \"vacations_remaining\": 1 }}\n",
    "\n",
    "Example 4:\n",
    "Input: \"Can John Smith's take 4 vacations?\"\n",
    "Output: {{ \"name\": \"John Smith\", \"vacations_remaining\": 4 }}\n",
    "\n",
    "'''\n",
    "\n",
    "PARAPHRASE_PROMPT = '''\n",
    "Task: Rewrite the provided text using formal language.\n",
    "\n",
    "Instructions: \n",
    "1. Please rephrase the text below in a formal tone, as if an HR representative were communicating with an employee.\n",
    "2. Keep it concise and straight to the point.\n",
    "3. No need to include extra text for context or explanation or Subject as in emails.\n",
    "\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "'''\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"documents\\\\\"\n",
    "\n",
    "WRITE_KEYWORDS = {'update', 'change', 'modify', 'delete', 'insert', 'set', 'assign'}\n",
    "READ_KEYWORDS  = {'get', 'show', 'find', 'retrieve', 'list', 'display', 'fetch'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GeminiEmbeddings(Embeddings):\n",
    "    def __init__(self, model: str = \"models/embedding-001\"):\n",
    "        \"\"\"Initialize with Gemini embedding model.\n",
    "        \n",
    "        Args:\n",
    "            model: The model name to use (default: \"models/embedding-001\")\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "    \n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        \"\"\"Generate an embedding for a single text.\"\"\"\n",
    "        try:\n",
    "            embedding = genai.embed_content(\n",
    "                model=self.model,\n",
    "                content=text,\n",
    "                task_type=\"retrieval_query\"\n",
    "            )\n",
    "            \n",
    "            # Extract embedding values\n",
    "            values = embedding[\"embedding\"]\n",
    "            \n",
    "            # Normalize the embedding vector using L2 normalization\n",
    "            norm = np.linalg.norm(values)\n",
    "            if norm == 0:\n",
    "                return values\n",
    "            normalized_embedding = [float(x) / norm for x in values]\n",
    "            return normalized_embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {e}\")\n",
    "            # Return a zero vector of appropriate dimension if there's an error\n",
    "            # Typical Gemini embedding dimensions are 768 or 1024\n",
    "            return [0.0] * 768\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        \"\"\"Generate embeddings for a list of texts.\"\"\"\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "class OllamaEmbeddings(Embeddings):\n",
    "    def __init__(self, model: str = \"mxbai-embed-large\"):\n",
    "        self.model = model\n",
    "        self.client = ollama.Client()\n",
    "    \n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        response = self.client.embeddings(model=self.model, prompt=text)\n",
    "        embedding = response[\"embedding\"]\n",
    "        # Normalize the embedding vector using L2 normalization\n",
    "        norm = np.linalg.norm(embedding)\n",
    "        if norm == 0:\n",
    "            return embedding\n",
    "        normalized_embedding = [x / norm for x in embedding]\n",
    "        return normalized_embedding\n",
    "\n",
    "    def embed_documents(self, texts: list[str]) -> list[list[float]]:\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    documents = []\n",
    "    # Load CSV files\n",
    "    for csv_file in glob.glob(os.path.join(DATA_PATH, \"*.csv\")):\n",
    "        loader = CSVLoader(file_path=csv_file)\n",
    "        docs = loader.load_and_split()\n",
    "        documents.extend(docs)\n",
    "    # Load XLSX files\n",
    "    for xlsx_file in glob.glob(os.path.join(DATA_PATH, \"*.xlsx\")):\n",
    "        loader = UnstructuredExcelLoader(file_path=xlsx_file)\n",
    "        docs = loader.load_and_split()\n",
    "        documents.extend(docs)\n",
    "    # Load PDF files\n",
    "    for pdf_file in glob.glob(os.path.join(DATA_PATH, \"*.pdf\")):\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        docs = loader.load_and_split()\n",
    "        documents.extend(docs)\n",
    "    # Load TXT files\n",
    "    for txt_file in glob.glob(os.path.join(DATA_PATH, \"*.txt\")):\n",
    "        with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            document = Document(page_content=content, metadata={'source': txt_file})\n",
    "            documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Use GeminiEmbeddings instead of OllamaEmbeddings\n",
    "    embeddings = GeminiEmbeddings()\n",
    "    # embeddings = OllamaEmbeddings()\n",
    "    db = Chroma(collection_name=\"foo\", embedding_function=embeddings, persist_directory=CHROMA_PATH)\n",
    "    \n",
    "    # Add chunks in batches\n",
    "    batch_size = 100  # Adjust this batch size as needed\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "        db.add_documents(batch_chunks)\n",
    "    \n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "    return db\n",
    "\n",
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    return save_to_chroma(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def force_delete_folder(folder_path, retries=3, delay=2):\n",
    "\n",
    "    # terminate_chromadb_server()\n",
    "\n",
    "    # Identify and terminate processes using the folder\n",
    "    for proc in psutil.process_iter(['pid', 'name']):\n",
    "        try:\n",
    "            for item in proc.open_files():\n",
    "                if item.path.startswith(folder_path):\n",
    "                    proc.terminate()  # or proc.kill() for a more forceful termination\n",
    "                    proc.wait()  # wait for the process to terminate\n",
    "                    print(f\"Terminated process {proc.pid} that was using the file {item.path}\")\n",
    "        except (psutil.NoSuchProcess, psutil.AccessDenied):\n",
    "            continue\n",
    "    \n",
    "    # Retry deletion with better error handling\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            shutil.rmtree(folder_path)\n",
    "            print(f\"Deleted folder: {folder_path}\")\n",
    "            break\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Folder not found: {folder_path}\")\n",
    "            break\n",
    "        except PermissionError:\n",
    "            print(f\"Permission denied: {folder_path}, retrying in {delay} seconds...\")\n",
    "            time.sleep(delay)  # wait before retrying\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting folder: {str(e)}\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"Failed to delete folder {folder_path} after {retries} attempts\")\n",
    "\n",
    "def rule_based_classify(query: str) -> str:\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    if ('vacation' in query_lower or 'holiday' in query_lower or \"sick-leave\" in query_lower or \"sick leave\" in query_lower or \"day off\" in query_lower or \"day-off\" in query_lower) and ('can' in query_lower or 'want' in query_lower):\n",
    "        return \"write\", 1\n",
    "\n",
    "    found_write = {kw for kw in WRITE_KEYWORDS if kw in query_lower}\n",
    "    found_read = {kw for kw in READ_KEYWORDS if kw in query_lower}\n",
    "    \n",
    "    if found_write and not found_read:\n",
    "        return \"write\", 0\n",
    "    if found_read and not found_write:\n",
    "        return \"read\", 0\n",
    "    return \"ambiguous\", 0\n",
    "\n",
    "def ml_based_classify(query: str) -> str:\n",
    "    prompt = '''\n",
    "    Task: Identify if the query is a read or write request.\n",
    "\n",
    "    Instructions: Answer only in \"read\" or \"write\" \n",
    "    Do not include anything else.\n",
    "\n",
    "    query: {question}\n",
    "    '''\n",
    "    client = ollama.Client()\n",
    "    model = 'mistral:latest'\n",
    "    response = client.generate(model, prompt.format(question=query))\n",
    "    return response['response'], 0\n",
    "\n",
    "def classify_query(query: str) -> str:\n",
    "    rule_result = rule_based_classify(query)\n",
    "    if rule_result != \"ambiguous\":\n",
    "        return rule_result\n",
    "    return ml_based_classify(query)\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"\n",
    "    Extracts valid JSON from text by finding content within outermost curly braces.\n",
    "    Returns the parsed JSON object or None if no valid JSON is found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find content between outermost curly braces using regex\n",
    "        # This handles nested braces and multiline JSON\n",
    "        pattern = r'\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\}))*\\}))*\\}'\n",
    "        matches = re.findall(pattern, text)\n",
    "        \n",
    "        if not matches:\n",
    "            return None\n",
    "        \n",
    "        # Try to parse each match as JSON and return the first valid one\n",
    "        for match in matches:\n",
    "            try:\n",
    "                return json.loads(match)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "                \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting JSON: {e}\")\n",
    "        return None\n",
    "\n",
    "def update_csv_from_json(json_data: Union[Dict, str], csv_path: str, backup: bool = True) -> bool:\n",
    "    \"\"\"\n",
    "    Update a CSV file based on the provided JSON data.\n",
    "    \n",
    "    Args:\n",
    "        json_data: JSON data as dictionary or string, containing ID or name for record lookup\n",
    "                  and additional fields to update\n",
    "        csv_path: Path to the CSV file\n",
    "        backup: Whether to create a backup of the original file before updating\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if update was successful, False otherwise\n",
    "        \n",
    "    Example JSON formats:\n",
    "        {\"ID\": 1011, \"salary\": 2300}\n",
    "        {\"name\": \"John Smith\", \"vacations_remaining\": 5}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse JSON if it's a string\n",
    "        if isinstance(json_data, str):\n",
    "            json_data = json.loads(json_data)\n",
    "        \n",
    "        # Validate JSON data\n",
    "        if not isinstance(json_data, dict):\n",
    "            print(\"Error: JSON data must be a dictionary\")\n",
    "            return False\n",
    "            \n",
    "        # Check if we have either ID or name to locate the record\n",
    "        has_id = \"ID\" in json_data\n",
    "        has_name = \"name\" in json_data\n",
    "        \n",
    "        if not (has_id or has_name):\n",
    "            print(\"Error: JSON must contain either 'ID' or 'name' to locate a record\")\n",
    "            return False\n",
    "        \n",
    "        # Create a backup if requested\n",
    "        if backup and os.path.exists(csv_path):\n",
    "            backup_path = f\"{csv_path}.bak\"\n",
    "            pd.read_csv(csv_path).to_csv(backup_path, index=False)\n",
    "            print(f\"Backup created at {backup_path}\")\n",
    "        \n",
    "        # Load CSV into DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Locate the record\n",
    "        record_found = False\n",
    "        if has_id:\n",
    "            id_value = json_data[\"ID\"]\n",
    "            mask = df[\"ID\"] == id_value\n",
    "            record_found = mask.any()\n",
    "            if not record_found:\n",
    "                print(f\"No record found with ID: {id_value}\")\n",
    "                return False\n",
    "        else:  # has_name\n",
    "            name_value = json_data[\"name\"]\n",
    "            mask = df[\"name\"] == name_value\n",
    "            record_found = mask.any()\n",
    "            if not record_found:\n",
    "                print(f\"No record found with name: {name_value}\")\n",
    "                return False\n",
    "        \n",
    "        # Update the record with each field in the JSON\n",
    "        update_count = 0\n",
    "        for key, value in json_data.items():\n",
    "            # Skip the identifier fields\n",
    "            if key in [\"ID\", \"name\"]:\n",
    "                continue\n",
    "                \n",
    "            # Check if the column exists\n",
    "            if key not in df.columns:\n",
    "                print(f\"Warning: Column '{key}' not found in CSV, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Update the value\n",
    "            df.loc[mask, key] = value\n",
    "            update_count += 1\n",
    "        \n",
    "        if update_count == 0:\n",
    "            print(\"No fields to update\")\n",
    "            return False\n",
    "            \n",
    "        # Save the updated DataFrame back to CSV\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"Successfully updated {update_count} fields for record\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating CSV: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def calculate_remaining_vacations(json_data: Union[Dict, str], csv_path: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Calculate the remaining vacation days after subtracting the requested amount.\n",
    "    \n",
    "    Args:\n",
    "        json_data: JSON data as dictionary or string, containing ID or name for record lookup\n",
    "                  and vacations_remaining field representing the requested vacation days\n",
    "        csv_path: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        int: The number of vacation days that would remain after the request, or None if error\n",
    "        \n",
    "    Example JSON formats:\n",
    "        {\"ID\": 10, \"vacations_remaining\": 5}\n",
    "        {\"name\": \"James Smith\", \"vacations_remaining\": 2}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse JSON if it's a string\n",
    "        if isinstance(json_data, str):\n",
    "            json_data = json.loads(json_data)\n",
    "        \n",
    "        # Validate JSON data\n",
    "        if not isinstance(json_data, dict):\n",
    "            print(\"Error: JSON data must be a dictionary\")\n",
    "            return None\n",
    "            \n",
    "        # Check if we have vacations_remaining field\n",
    "        if \"vacations_remaining\" not in json_data:\n",
    "            print(\"Error: JSON must contain 'vacations_remaining' field\")\n",
    "            return None\n",
    "            \n",
    "        # Check if we have either ID or name to locate the record\n",
    "        has_id = \"ID\" in json_data\n",
    "        has_name = \"name\" in json_data\n",
    "        \n",
    "        if not (has_id or has_name):\n",
    "            print(\"Error: JSON must contain either 'ID' or 'name' to locate a record\")\n",
    "            return None\n",
    "        \n",
    "        # Load CSV into DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Locate the record\n",
    "        if has_id:\n",
    "            id_value = json_data[\"ID\"]\n",
    "            mask = df[\"ID\"] == id_value\n",
    "            if not mask.any():\n",
    "                print(f\"No record found with ID: {id_value}\")\n",
    "                return None\n",
    "        else:  # has_name\n",
    "            name_value = json_data[\"name\"]\n",
    "            mask = df[\"name\"] == name_value\n",
    "            if not mask.any():\n",
    "                print(f\"No record found with name: {name_value}\")\n",
    "                return None\n",
    "        \n",
    "        # Get the current vacations_remaining value from CSV\n",
    "        current_vacations = df.loc[mask, \"vacations_remaining\"].values[0]\n",
    "        \n",
    "        # Get the requested vacations from JSON\n",
    "        requested_vacations = json_data[\"vacations_remaining\"]\n",
    "        \n",
    "        # Calculate the remaining vacations\n",
    "        remaining_vacations = current_vacations - requested_vacations\n",
    "        \n",
    "        return remaining_vacations\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating remaining vacations: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_record_from_json(json_data: Union[Dict, str], csv_path: str) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve a record from a CSV file based on ID or name in the provided JSON.\n",
    "    \n",
    "    Args:\n",
    "        json_data: JSON data as dictionary or string containing ID or name for lookup\n",
    "        csv_path: Path to the CSV file\n",
    "        \n",
    "    Returns:\n",
    "        Dict: The complete record as a dictionary, or None if not found\n",
    "        \n",
    "    Example JSON formats:\n",
    "        {\"ID\": 6}\n",
    "        {\"name\": \"James Smith\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Parse JSON if it's a string\n",
    "        if isinstance(json_data, str):\n",
    "            json_data = json.loads(json_data)\n",
    "        \n",
    "        # Validate JSON data\n",
    "        if not isinstance(json_data, dict):\n",
    "            print(\"Error: JSON data must be a dictionary\")\n",
    "            return None\n",
    "            \n",
    "        # Check if we have either ID or name to locate the record\n",
    "        has_id = \"ID\" in json_data\n",
    "        has_name = \"name\" in json_data\n",
    "        \n",
    "        if not (has_id or has_name):\n",
    "            print(\"Error: JSON must contain either 'ID' or 'name' to locate a record\")\n",
    "            return None\n",
    "        \n",
    "        # Load CSV into DataFrame\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Locate the record\n",
    "        if has_id:\n",
    "            id_value = json_data[\"ID\"]\n",
    "            mask = df[\"ID\"] == id_value\n",
    "            if not mask.any():\n",
    "                print(f\"No record found with ID: {id_value}\")\n",
    "                return None\n",
    "        else:  # has_name\n",
    "            name_value = json_data[\"name\"]\n",
    "            mask = df[\"name\"] == name_value\n",
    "            if not mask.any():\n",
    "                print(f\"No record found with name: {name_value}\")\n",
    "                return None\n",
    "        \n",
    "        # Convert the record to a dictionary\n",
    "        record = df.loc[mask].iloc[0].to_dict()\n",
    "        \n",
    "        # Ensure numeric fields are properly typed (not numpy types)\n",
    "        for key, value in record.items():\n",
    "            if hasattr(value, 'item'):  # Check if it's a numpy type\n",
    "                record[key] = value.item()  # Convert numpy type to native Python type\n",
    "        \n",
    "        return record\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving record: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def predict(model, query_text, db, csv_file, combined=False):\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=4)\n",
    "    \n",
    "    if len(results) == 0 or results[0][1] < 0.2:\n",
    "        return \"Unable to find matching results.\"\n",
    "\n",
    "    query_context = model.generate_content([\n",
    "                {\"role\": \"user\", \"parts\": [FORMATTING_PROMPT]},\n",
    "                {\"role\": \"user\", \"parts\": [query_text]}\n",
    "            ]).text\n",
    "    extracted_json = extract_json_from_text(query_context)\n",
    "    record = get_record_from_json(extracted_json, csv_file)\n",
    "\n",
    "    if record is None:\n",
    "        context_text = \"\\n---\\n\".join([doc.page_content for doc, _score in results])\n",
    "    elif record is not None and combined:\n",
    "        context_text = \"\\n---\\n\".join([doc.page_content for doc, _score in results])\n",
    "        context_text += \"\\n---\\n\" + str(record)\n",
    "    else:\n",
    "        context_text = record\n",
    "        \n",
    "    prompt_template = ChatPromptTemplate.from_template(CONTEXT_PROMPT)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    \n",
    "    answer = model.generate_content([\n",
    "                    {\"role\": \"user\", \"parts\": [prompt]}\n",
    "                ]).text\n",
    "\n",
    "    sources = [doc.metadata['source'] for doc, _score in results]\n",
    "\n",
    "    return answer, sources\n",
    "\n",
    "def update_db(db, retries=1, delay=0):\n",
    "    print(f\"\\nUpdating the Database....\")\n",
    "    db.delete_collection()\n",
    "    force_delete_folder(folder_path=CHROMA_PATH, retries=retries, delay=delay)\n",
    "    db = generate_data_store()\n",
    "    print(f\"\\nDatabase Updated.\")\n",
    "    return db\n",
    "\n",
    "def rag_chatbot(model, db, csv_file, formalize_flag=False):\n",
    "\n",
    "    print(\"Chatbot: Hello! How can I help you today? (Type 'exit' to end the chat)\")\n",
    "    while True:\n",
    "        query_text = input(\"You: \")\n",
    "        if query_text.lower() == \"exit\":\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        classification, vaca_flag = classify_query(query_text)\n",
    "\n",
    "        if classification == \"write\":\n",
    "            output = model.generate_content([\n",
    "                {\"role\": \"user\", \"parts\": [FORMATTING_PROMPT]},\n",
    "                {\"role\": \"user\", \"parts\": [query_text]}\n",
    "            ]).text\n",
    "            extracted_json = extract_json_from_text(output)\n",
    "\n",
    "            if extracted_json != {}:\n",
    "                if vaca_flag:\n",
    "                    remaining = calculate_remaining_vacations(extracted_json, csv_file)\n",
    "                    if remaining is not None:\n",
    "                        print(f\"Remaining vacations after request: {remaining}\")\n",
    "                        if remaining < 0:\n",
    "                            response = f\"Not enough vacation days available! \\nNo vacation granted. You will have {remaining} days remaining.\"\n",
    "                        else:\n",
    "                            response = f\"Yes, You can take the vacation. You will have {remaining} days remaining.\"\n",
    "\n",
    "                else:\n",
    "                    result = update_csv_from_json(extracted_json, csv_file, backup=False)\n",
    "\n",
    "                    if result:\n",
    "                        response = \"Successfully updated the record.\"\n",
    "                        sources = [csv_file]\n",
    "                    else:\n",
    "                        response = \"Failed to update the record.\"\n",
    "                        sources = []\n",
    "\n",
    "                    db = update_db(db, retries=1, delay=0)\n",
    "\n",
    "            else:\n",
    "                response = \"Unable to extract Information from text. Please try phrasing the text in a more clear and structured manner.\"\n",
    "                sources = []\n",
    "\n",
    "            if formalize_flag:\n",
    "                response = model.generate_content([\n",
    "                    {\"role\": \"user\", \"parts\": [PARAPHRASE_PROMPT.format(text=response)]},\n",
    "                ]).text\n",
    "            \n",
    "        else:\n",
    "            response, sources = predict(model, query_text, db, csv_file)\n",
    "\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        print(f\"Sources: {sources}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 99 documents into 99 chunks.\n",
      "Saved 99 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the generative model\n",
    "model = genai.GenerativeModel('gemini-2.0-flash')\n",
    "\n",
    "# Delete the chromadb folder if it exists\n",
    "chromadb_path = 'chroma'\n",
    "if os.path.exists(chromadb_path):\n",
    "    shutil.rmtree(chromadb_path)\n",
    "\n",
    "db = generate_data_store()\n",
    "csv_file = \"documents\\\\employees.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hello! How can I help you today? (Type 'exit' to end the chat)\n",
      "Chatbot: Rebecca Schwartz is an employee with an annual salary of $124,076. As of the current context, she has 13 vacation days remaining. Her join date, indicating the start of her employment, is August 15, 2017.\n",
      "\n",
      "Sources: ['documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv']\n",
      "Chatbot: Based on the provided context, Rebecca Schwartz has 13 vacations remaining.\n",
      "\n",
      "Sources: ['documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv']\n",
      "Remaining vacations after request: 10\n",
      "\n",
      "Updating the Database....\n",
      "Permission denied: chroma, retrying in 0 seconds...\n",
      "Failed to delete folder chroma after 1 attempts\n",
      "Split 99 documents into 99 chunks.\n",
      "Saved 99 chunks to chroma.\n",
      "\n",
      "Database Updated.\n",
      "Chatbot: Your vacation request has been approved. You will have a remaining balance of 10 vacation days.\n",
      "\n",
      "Sources: ['documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv']\n",
      "Error: JSON must contain either 'ID' or 'name' to locate a record\n",
      "Chatbot: Rebecca Schwartz is an employee with an ID of 5. Her salary is $124,076, and she has 13 vacation days remaining. She joined the company on August 15, 2017.\n",
      "\n",
      "Sources: ['documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv']\n",
      "Successfully updated 1 fields for record\n",
      "\n",
      "Updating the Database....\n",
      "Permission denied: chroma, retrying in 0 seconds...\n",
      "Failed to delete folder chroma after 1 attempts\n",
      "Split 99 documents into 99 chunks.\n",
      "Saved 99 chunks to chroma.\n",
      "\n",
      "Database Updated.\n",
      "Chatbot: The record has been updated successfully.\n",
      "\n",
      "Sources: ['documents\\\\employees.csv']\n",
      "Error: JSON must contain either 'ID' or 'name' to locate a record\n",
      "Chatbot: Rebecca Schwartz's employee ID is 5. Her salary is $124,076, and she has 10 vacation days remaining. She joined the company on 2017-08-15.\n",
      "\n",
      "Sources: ['documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv', 'documents\\\\employees.csv']\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rag_chatbot(model, db, csv_file, formalize_flag=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
